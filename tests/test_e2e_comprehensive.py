#!/usr/bin/env python3
"""
Paper Parser å…¨é¢ç«¯åˆ°ç«¯æµ‹è¯•å¥—ä»¶
æµ‹è¯•æ•´ä¸ªç³»ç»Ÿçš„å®Œæ•´å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬ç¼“å­˜ç­–ç•¥ã€é”™è¯¯å¤„ç†å’Œæ€§èƒ½éªŒè¯
"""
import asyncio
import aiohttp
import json
import time
import random
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import pytest
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))


@dataclass
class TestMetrics:
    """æµ‹è¯•æŒ‡æ ‡"""
    test_name: str
    status: str  # PASS, FAIL, WARN, SKIP
    timestamp: str
    duration: float
    response_time: float
    cache_hit: Optional[str] = None
    error: Optional[str] = None
    details: Optional[Dict[str, Any]] = None


@dataclass
class SystemHealthCheck:
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥ç»“æœ"""
    redis_available: bool
    neo4j_available: bool
    s2_api_available: bool
    api_service_running: bool
    overall_status: str


class ComprehensiveE2ETester:
    """å…¨é¢çš„ç«¯åˆ°ç«¯æµ‹è¯•å™¨"""
    
    def __init__(self, base_url: str = "http://127.0.0.1:8000"):
        self.base_url = base_url
        self.api_prefix = "/api/v1"
        self.session = None
        self.metrics: List[TestMetrics] = []
        
        # æµ‹è¯•æ•°æ®é›†
        self.test_papers = {
            "attention_paper": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",  # Attention is All You Need
            "bert_paper": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",  # BERT
            "gpt_paper": "cd18800a0fe0b668a1cc19f2ec95b2a0c3a0e3b8",  # GPT
            "invalid_id": "invalid_paper_id_12345",
            "nonexistent_id": "0000000000000000000000000000000000000000"
        }
        
        self.test_queries = [
            "attention is all you need",
            "transformer neural network",
            "machine learning",
            "deep learning",
            "natural language processing",
            "computer vision",
            "artificial intelligence"
        ]
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        timeout = aiohttp.ClientTimeout(total=60, connect=10, sock_read=30)
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            headers={
                "Content-Type": "application/json",
                "User-Agent": "Paper-Parser-E2E-Test/1.0"
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    def _extract_cache_info(self, headers: Dict[str, str]) -> str:
        """æå–ç¼“å­˜ä¿¡æ¯"""
        cache_info = headers.get('x-cache', 'UNKNOWN')
        if cache_info == 'UNKNOWN':
            # å°è¯•å…¶ä»–å¯èƒ½çš„ç¼“å­˜å¤´
            for key in headers:
                if 'cache' in key.lower():
                    cache_info = f"{key}:{headers[key]}"
                    break
        return cache_info
    
    def _extract_response_time(self, headers: Dict[str, str]) -> float:
        """æå–å“åº”æ—¶é—´"""
        process_time = headers.get('x-process-time', '0')
        try:
            return float(process_time.replace('ms', '').replace('s', ''))
        except (ValueError, AttributeError):
            return 0.0
    
    async def _make_request(self, method: str, endpoint: str, **kwargs) -> Tuple[int, Dict, Dict[str, str]]:
        """å‘é€HTTPè¯·æ±‚å¹¶è¿”å›çŠ¶æ€ç ã€æ•°æ®å’Œå¤´éƒ¨"""
        url = f"{self.base_url}{self.api_prefix}{endpoint}"
        
        try:
            async with self.session.request(method, url, **kwargs) as response:
                headers = dict(response.headers)
                try:
                    data = await response.json()
                except aiohttp.ContentTypeError:
                    data = {"error": "Invalid JSON response", "text": await response.text()}
                
                return response.status, data, headers
        except asyncio.TimeoutError:
            raise Exception("Request timeout")
        except aiohttp.ClientError as e:
            raise Exception(f"Client error: {str(e)}")
    
    async def _run_test(self, test_name: str, test_func) -> TestMetrics:
        """è¿è¡Œå•ä¸ªæµ‹è¯•å¹¶è®°å½•æŒ‡æ ‡"""
        print(f"\nğŸ§ª {test_name}")
        start_time = time.time()
        
        try:
            result = await test_func()
            duration = time.time() - start_time
            
            metrics = TestMetrics(
                test_name=test_name,
                status="PASS",
                timestamp=datetime.now().isoformat(),
                duration=duration,
                response_time=result.get('response_time', 0),
                cache_hit=result.get('cache_hit'),
                details=result
            )
            
            print(f"âœ… {test_name}: PASS ({duration:.2f}s)")
            if result.get('cache_hit'):
                print(f"   ğŸ“¦ ç¼“å­˜: {result['cache_hit']}")
            if result.get('response_time'):
                print(f"   â±ï¸  å“åº”æ—¶é—´: {result['response_time']:.3f}s")
            
        except AssertionError as e:
            duration = time.time() - start_time
            metrics = TestMetrics(
                test_name=test_name,
                status="FAIL",
                timestamp=datetime.now().isoformat(),
                duration=duration,
                response_time=0,
                error=str(e)
            )
            print(f"âŒ {test_name}: FAIL - {e}")
            
        except Exception as e:
            duration = time.time() - start_time
            metrics = TestMetrics(
                test_name=test_name,
                status="WARN",
                timestamp=datetime.now().isoformat(),
                duration=duration,
                response_time=0,
                error=f"{type(e).__name__}: {str(e)}"
            )
            print(f"âš ï¸  {test_name}: WARN - {e}")
        
        self.metrics.append(metrics)
        return metrics
    
    # ========== ç³»ç»Ÿå¥åº·æ£€æŸ¥ ==========
    
    async def test_system_health(self) -> Dict[str, Any]:
        """å…¨é¢çš„ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
        health_results = {}
        
        # 1. åŸºç¡€å¥åº·æ£€æŸ¥
        try:
            status, data, headers = await self._make_request("GET", "/health")
            health_results['basic_health'] = {
                'status_code': status,
                'success': status == 200,
                'response': data
            }
        except Exception as e:
            health_results['basic_health'] = {
                'status_code': 0,
                'success': False,
                'error': str(e)
            }
        
        # 2. è¯¦ç»†å¥åº·æ£€æŸ¥
        try:
            status, data, headers = await self._make_request("GET", "/health/detailed")
            health_results['detailed_health'] = {
                'status_code': status,
                'success': status == 200,
                'response': data,
                'response_time': self._extract_response_time(headers)
            }
            
            # åˆ†æå„ç»„ä»¶çŠ¶æ€
            if status == 200 and isinstance(data, dict):
                components = data.get('data', {})
                health_results['components'] = {
                    'redis': components.get('redis', {}).get('status') == 'healthy',
                    'neo4j': components.get('neo4j', {}).get('status') == 'healthy',
                    's2_api': components.get('s2_api', {}).get('status') == 'healthy',
                }
        except Exception as e:
            health_results['detailed_health'] = {
                'status_code': 0,
                'success': False,
                'error': str(e)
            }
        
        # 3. æ ¹è·¯å¾„æ£€æŸ¥
        try:
            root_response = await self.session.get(f"{self.base_url}/")
            root_data = await root_response.json()
            health_results['root_endpoint'] = {
                'status_code': root_response.status,
                'success': root_response.status == 200,
                'service_name': root_data.get('data', {}).get('service', 'Unknown')
            }
        except Exception as e:
            health_results['root_endpoint'] = {
                'status_code': 0,
                'success': False,
                'error': str(e)
            }
        
        # æ•´ä½“å¥åº·è¯„ä¼°
        overall_healthy = all([
            health_results.get('basic_health', {}).get('success', False),
            health_results.get('detailed_health', {}).get('success', False),
            health_results.get('root_endpoint', {}).get('success', False)
        ])
        
        health_results['overall_status'] = 'HEALTHY' if overall_healthy else 'UNHEALTHY'
        
        return health_results
    
    # ========== æ ¸å¿ƒAPIåŠŸèƒ½æµ‹è¯• ==========
    
    async def test_paper_search_comprehensive(self) -> Dict[str, Any]:
        """å…¨é¢çš„è®ºæ–‡æœç´¢æµ‹è¯•"""
        results = {}
        
        # 1. åŸºç¡€æœç´¢æµ‹è¯•
        query = "attention is all you need"
        status, data, headers = await self._make_request(
            "GET", "/paper/search",
            params={"query": query, "limit": 5}
        )
        
        assert status == 200, f"æœç´¢APIçŠ¶æ€ç å¼‚å¸¸: {status}"
        assert data["success"] is True, "æœç´¢å“åº”successå­—æ®µä¸ä¸ºTrue"
        
        search_data = data["data"]
        assert "papers" in search_data, "æœç´¢ç»“æœç¼ºå°‘paperså­—æ®µ"
        assert "total" in search_data, "æœç´¢ç»“æœç¼ºå°‘totalå­—æ®µ"
        assert len(search_data["papers"]) > 0, "æœç´¢ç»“æœä¸ºç©º"
        
        results['basic_search'] = {
            'query': query,
            'results_count': len(search_data["papers"]),
            'total_available': search_data["total"],
            'response_time': self._extract_response_time(headers),
            'cache_hit': self._extract_cache_info(headers)
        }
        
        # 2. å¸¦è¿‡æ»¤æ¡ä»¶çš„æœç´¢
        status, data, headers = await self._make_request(
            "GET", "/paper/search",
            params={
                "query": "transformer",
                "year": "2017-2020",
                "limit": 10,
                "fields": "paperId,title,authors,year,citationCount"
            }
        )
        
        assert status == 200, f"è¿‡æ»¤æœç´¢çŠ¶æ€ç å¼‚å¸¸: {status}"
        filtered_papers = data["data"]["papers"]
        
        # éªŒè¯å¹´ä»½è¿‡æ»¤
        for paper in filtered_papers:
            if paper.get("year"):
                assert 2017 <= paper["year"] <= 2020, f"å¹´ä»½è¿‡æ»¤å¤±æ•ˆ: {paper['year']}"
        
        results['filtered_search'] = {
            'query': 'transformer',
            'year_filter': '2017-2020',
            'results_count': len(filtered_papers),
            'years_found': [p.get("year") for p in filtered_papers if p.get("year")],
            'response_time': self._extract_response_time(headers)
        }
        
        # 3. åˆ†é¡µæµ‹è¯•
        page1_status, page1_data, _ = await self._make_request(
            "GET", "/paper/search",
            params={"query": "machine learning", "offset": 0, "limit": 5}
        )
        
        page2_status, page2_data, _ = await self._make_request(
            "GET", "/paper/search",
            params={"query": "machine learning", "offset": 5, "limit": 5}
        )
        
        assert page1_status == 200 and page2_status == 200, "åˆ†é¡µæœç´¢å¤±è´¥"
        
        page1_papers = page1_data["data"]["papers"]
        page2_papers = page2_data["data"]["papers"]
        
        # éªŒè¯åˆ†é¡µä¸é‡å¤
        page1_ids = {p["paperId"] for p in page1_papers}
        page2_ids = {p["paperId"] for p in page2_papers}
        overlap = page1_ids & page2_ids
        
        results['pagination'] = {
            'page1_count': len(page1_papers),
            'page2_count': len(page2_papers),
            'overlap_count': len(overlap),
            'pagination_working': len(overlap) == 0
        }
        
        return results
    
    async def test_paper_detail_comprehensive(self) -> Dict[str, Any]:
        """å…¨é¢çš„è®ºæ–‡è¯¦æƒ…æµ‹è¯•"""
        results = {}
        paper_id = self.test_papers["attention_paper"]
        
        # 1. åŸºç¡€è¯¦æƒ…è·å–
        status, data, headers = await self._make_request("GET", f"/paper/{paper_id}")
        
        assert status == 200, f"è®ºæ–‡è¯¦æƒ…çŠ¶æ€ç å¼‚å¸¸: {status}"
        assert data["success"] is True, "è¯¦æƒ…å“åº”successå­—æ®µä¸ä¸ºTrue"
        
        paper_data = data["data"]
        required_fields = ["paperId", "title", "authors", "year", "citationCount"]
        for field in required_fields:
            assert field in paper_data, f"è®ºæ–‡è¯¦æƒ…ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
        
        results['basic_detail'] = {
            'paper_id': paper_id,
            'title': paper_data.get("title"),
            'authors_count': len(paper_data.get("authors", [])),
            'year': paper_data.get("year"),
            'citation_count': paper_data.get("citationCount"),
            'response_time': self._extract_response_time(headers),
            'cache_hit': self._extract_cache_info(headers)
        }
        
        # 2. å­—æ®µè¿‡æ»¤æµ‹è¯•
        status, data, headers = await self._make_request(
            "GET", f"/paper/{paper_id}",
            params={"fields": "paperId,title,year"}
        )
        
        assert status == 200, "å­—æ®µè¿‡æ»¤è¯·æ±‚å¤±è´¥"
        filtered_data = data["data"]
        
        # éªŒè¯åªè¿”å›è¯·æ±‚çš„å­—æ®µ
        expected_fields = {"paperId", "title", "year"}
        actual_fields = set(filtered_data.keys())
        
        results['field_filtering'] = {
            'requested_fields': list(expected_fields),
            'returned_fields': list(actual_fields),
            'filtering_working': expected_fields.issubset(actual_fields),
            'response_time': self._extract_response_time(headers)
        }
        
        # 3. ç¼“å­˜æµ‹è¯• - è¿ç»­è¯·æ±‚åŒä¸€è®ºæ–‡
        cache_times = []
        for i in range(3):
            start = time.time()
            status, data, headers = await self._make_request("GET", f"/paper/{paper_id}")
            cache_times.append(time.time() - start)
            await asyncio.sleep(0.1)  # çŸ­æš‚é—´éš”
        
        results['cache_performance'] = {
            'request_times': cache_times,
            'avg_time': sum(cache_times) / len(cache_times),
            'speed_improvement': cache_times[0] / cache_times[-1] if cache_times[-1] > 0 else 1
        }
        
        return results
    
    async def test_paper_relations_comprehensive(self) -> Dict[str, Any]:
        """å…¨é¢çš„è®ºæ–‡å…³ç³»æµ‹è¯•ï¼ˆå¼•ç”¨å’Œå‚è€ƒæ–‡çŒ®ï¼‰"""
        results = {}
        paper_id = self.test_papers["attention_paper"]
        
        # 1. å¼•ç”¨æµ‹è¯•
        status, data, headers = await self._make_request(
            "GET", f"/paper/{paper_id}/citations",
            params={"limit": 10}
        )
        
        assert status == 200, f"å¼•ç”¨APIçŠ¶æ€ç å¼‚å¸¸: {status}"
        assert data["success"] is True, "å¼•ç”¨å“åº”successå­—æ®µä¸ä¸ºTrue"
        
        citations_data = data["data"]
        assert "citations" in citations_data, "å¼•ç”¨æ•°æ®ç¼ºå°‘citationså­—æ®µ"
        
        results['citations'] = {
            'paper_id': paper_id,
            'citations_found': len(citations_data["citations"]),
            'total_citations': citations_data.get("total", 0),
            'response_time': self._extract_response_time(headers),
            'cache_hit': self._extract_cache_info(headers)
        }
        
        # 2. å‚è€ƒæ–‡çŒ®æµ‹è¯•
        status, data, headers = await self._make_request(
            "GET", f"/paper/{paper_id}/references",
            params={"limit": 10}
        )
        
        assert status == 200, f"å‚è€ƒæ–‡çŒ®APIçŠ¶æ€ç å¼‚å¸¸: {status}"
        assert data["success"] is True, "å‚è€ƒæ–‡çŒ®å“åº”successå­—æ®µä¸ä¸ºTrue"
        
        references_data = data["data"]
        assert "references" in references_data, "å‚è€ƒæ–‡çŒ®æ•°æ®ç¼ºå°‘referenceså­—æ®µ"
        
        results['references'] = {
            'paper_id': paper_id,
            'references_found': len(references_data["references"]),
            'total_references': references_data.get("total", 0),
            'response_time': self._extract_response_time(headers),
            'cache_hit': self._extract_cache_info(headers)
        }
        
        # 3. åˆ†é¡µæµ‹è¯•
        citations_page1_status, citations_page1_data, _ = await self._make_request(
            "GET", f"/paper/{paper_id}/citations",
            params={"offset": 0, "limit": 5}
        )
        
        citations_page2_status, citations_page2_data, _ = await self._make_request(
            "GET", f"/paper/{paper_id}/citations",
            params={"offset": 5, "limit": 5}
        )
        
        assert citations_page1_status == 200 and citations_page2_status == 200, "å¼•ç”¨åˆ†é¡µå¤±è´¥"
        
        results['citations_pagination'] = {
            'page1_count': len(citations_page1_data["data"]["citations"]),
            'page2_count': len(citations_page2_data["data"]["citations"]),
            'pagination_working': True  # åŸºç¡€æ£€æŸ¥é€šè¿‡
        }
        
        return results
    
    async def test_batch_operations(self) -> Dict[str, Any]:
        """æ‰¹é‡æ“ä½œæµ‹è¯•"""
        results = {}
        
        # 1. å‡†å¤‡æµ‹è¯•æ•°æ® - å…ˆæœç´¢è·å–ä¸€äº›è®ºæ–‡ID
        status, data, _ = await self._make_request(
            "GET", "/paper/search",
            params={"query": "machine learning", "limit": 5}
        )
        
        assert status == 200, "æœç´¢å‡†å¤‡æ•°æ®å¤±è´¥"
        papers = data["data"]["papers"]
        paper_ids = [paper["paperId"] for paper in papers[:3]]  # å–å‰3ä¸ª
        
        # 2. æ‰¹é‡è·å–æµ‹è¯•
        start_time = time.time()
        status, data, headers = await self._make_request(
            "POST", "/paper/batch",
            json={
                "ids": paper_ids,
                "fields": "paperId,title,authors,year,citationCount"
            }
        )
        batch_time = time.time() - start_time
        
        assert status == 200, f"æ‰¹é‡è·å–çŠ¶æ€ç å¼‚å¸¸: {status}"
        assert data["success"] is True, "æ‰¹é‡è·å–å“åº”successå­—æ®µä¸ä¸ºTrue"
        
        batch_data = data["data"]
        assert isinstance(batch_data, list), "æ‰¹é‡æ•°æ®åº”è¯¥æ˜¯åˆ—è¡¨"
        
        results['batch_retrieval'] = {
            'requested_count': len(paper_ids),
            'returned_count': len(batch_data),
            'success_rate': len([p for p in batch_data if p is not None]) / len(paper_ids),
            'batch_time': batch_time,
            'avg_time_per_paper': batch_time / len(paper_ids) if paper_ids else 0,
            'response_time': self._extract_response_time(headers)
        }
        
        # 3. å¤§æ‰¹é‡æµ‹è¯• (æµ‹è¯•é™åˆ¶)
        large_batch_ids = paper_ids * 50  # åˆ›å»º150ä¸ªIDçš„æ‰¹é‡è¯·æ±‚
        
        status, data, headers = await self._make_request(
            "POST", "/paper/batch",
            json={"ids": large_batch_ids[:100]}  # æµ‹è¯•100ä¸ª
        )
        
        results['large_batch'] = {
            'requested_count': 100,
            'status_code': status,
            'success': status == 200,
            'response_time': self._extract_response_time(headers)
        }
        
        return results
    
    # ========== é”™è¯¯å¤„ç†å’Œè¾¹ç•Œæµ‹è¯• ==========
    
    async def test_error_handling_comprehensive(self) -> Dict[str, Any]:
        """å…¨é¢çš„é”™è¯¯å¤„ç†æµ‹è¯•"""
        results = {}
        
        # 1. æ— æ•ˆè®ºæ–‡IDæµ‹è¯•
        invalid_cases = [
            ("too_short", "12345"),
            ("invalid_format", self.test_papers["invalid_id"]),
            ("nonexistent", self.test_papers["nonexistent_id"]),
            ("special_chars", "invalid@#$%^&*()"),
            ("empty_string", "")
        ]
        
        invalid_results = {}
        for case_name, invalid_id in invalid_cases:
            if invalid_id:  # è·³è¿‡ç©ºå­—ç¬¦ä¸²æµ‹è¯•ï¼Œä¼šè¢«è·¯ç”±æ‹’ç»
                status, data, _ = await self._make_request("GET", f"/paper/{invalid_id}")
                invalid_results[case_name] = {
                    'paper_id': invalid_id,
                    'status_code': status,
                    'success': data.get("success", True),
                    'error_message': data.get("message", "No message")
                }
        
        results['invalid_paper_ids'] = invalid_results
        
        # 2. æœç´¢é”™è¯¯æµ‹è¯•
        search_error_cases = [
            ("empty_query", {"query": "", "limit": 5}),
            ("negative_offset", {"query": "test", "offset": -1}),
            ("excessive_limit", {"query": "test", "limit": 1000}),
            ("invalid_year", {"query": "test", "year": "invalid"}),
        ]
        
        search_errors = {}
        for case_name, params in search_error_cases:
            try:
                status, data, _ = await self._make_request("GET", "/paper/search", params=params)
                search_errors[case_name] = {
                    'params': params,
                    'status_code': status,
                    'success': data.get("success", True),
                    'error_handled': status in [400, 422]
                }
            except Exception as e:
                search_errors[case_name] = {
                    'params': params,
                    'error': str(e),
                    'error_handled': True
                }
        
        results['search_errors'] = search_errors
        
        # 3. æ‰¹é‡è¯·æ±‚é”™è¯¯æµ‹è¯•
        batch_error_cases = [
            ("empty_ids", {"ids": []}),
            ("too_many_ids", {"ids": ["test"] * 600}),  # è¶…è¿‡é™åˆ¶
            ("invalid_json", "invalid json"),
        ]
        
        batch_errors = {}
        for case_name, payload in batch_error_cases:
            try:
                if case_name == "invalid_json":
                    # å‘é€æ— æ•ˆJSON
                    url = f"{self.base_url}{self.api_prefix}/paper/batch"
                    async with self.session.post(url, data=payload) as response:
                        status = response.status
                        try:
                            data = await response.json()
                        except:
                            data = {"error": "Invalid JSON"}
                else:
                    status, data, _ = await self._make_request("POST", "/paper/batch", json=payload)
                
                batch_errors[case_name] = {
                    'payload_type': type(payload).__name__,
                    'status_code': status,
                    'error_handled': status in [400, 422, 413]
                }
            except Exception as e:
                batch_errors[case_name] = {
                    'error': str(e),
                    'error_handled': True
                }
        
        results['batch_errors'] = batch_errors
        
        return results
    
    # ========== æ€§èƒ½å’Œè´Ÿè½½æµ‹è¯• ==========
    
    async def test_performance_comprehensive(self) -> Dict[str, Any]:
        """å…¨é¢çš„æ€§èƒ½æµ‹è¯•"""
        results = {}
        
        # 1. å¹¶å‘æœç´¢æµ‹è¯•
        concurrent_queries = [
            "machine learning",
            "deep learning", 
            "neural network",
            "artificial intelligence",
            "computer vision"
        ]
        
        async def concurrent_search(query):
            start = time.time()
            status, data, headers = await self._make_request(
                "GET", "/paper/search",
                params={"query": query, "limit": 5}
            )
            return {
                'query': query,
                'status': status,
                'duration': time.time() - start,
                'success': status == 200,
                'results_count': len(data.get("data", {}).get("papers", [])) if status == 200 else 0
            }
        
        # å¹¶å‘æ‰§è¡Œæœç´¢
        start_concurrent = time.time()
        concurrent_results = await asyncio.gather(*[
            concurrent_search(query) for query in concurrent_queries
        ])
        total_concurrent_time = time.time() - start_concurrent
        
        results['concurrent_search'] = {
            'queries_count': len(concurrent_queries),
            'total_time': total_concurrent_time,
            'avg_time_per_query': total_concurrent_time / len(concurrent_queries),
            'all_successful': all(r['success'] for r in concurrent_results),
            'individual_results': concurrent_results
        }
        
        # 2. ç¼“å­˜æ€§èƒ½æµ‹è¯•
        paper_id = self.test_papers["attention_paper"]
        cache_test_results = []
        
        # é¦–æ¬¡è¯·æ±‚ï¼ˆå¯èƒ½è§¦å‘ç¼“å­˜ï¼‰
        start = time.time()
        status, data, headers = await self._make_request("GET", f"/paper/{paper_id}")
        first_request_time = time.time() - start
        
        # åç»­è¯·æ±‚ï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰
        for i in range(5):
            start = time.time()
            status, data, headers = await self._make_request("GET", f"/paper/{paper_id}")
            cache_test_results.append({
                'request_num': i + 2,
                'duration': time.time() - start,
                'cache_info': self._extract_cache_info(headers)
            })
            await asyncio.sleep(0.1)
        
        results['cache_performance'] = {
            'first_request_time': first_request_time,
            'cached_requests': cache_test_results,
            'avg_cached_time': sum(r['duration'] for r in cache_test_results) / len(cache_test_results),
            'cache_speedup': first_request_time / (sum(r['duration'] for r in cache_test_results) / len(cache_test_results))
        }
        
        # 3. å¤§æ•°æ®é‡æµ‹è¯•
        large_query_start = time.time()
        status, data, headers = await self._make_request(
            "GET", "/paper/search",
            params={"query": "machine learning", "limit": 100}
        )
        large_query_time = time.time() - large_query_start
        
        results['large_dataset'] = {
            'query': 'machine learning',
            'limit': 100,
            'duration': large_query_time,
            'status': status,
            'results_returned': len(data.get("data", {}).get("papers", [])) if status == 200 else 0,
            'response_time': self._extract_response_time(headers)
        }
        
        return results
    
    # ========== æ•°æ®ä¸€è‡´æ€§æµ‹è¯• ==========
    
    async def test_data_consistency(self) -> Dict[str, Any]:
        """æ•°æ®ä¸€è‡´æ€§æµ‹è¯•"""
        results = {}
        paper_id = self.test_papers["attention_paper"]
        
        # 1. åŒä¸€è®ºæ–‡å¤šæ¬¡è¯·æ±‚æ•°æ®ä¸€è‡´æ€§
        requests_data = []
        for i in range(3):
            status, data, _ = await self._make_request("GET", f"/paper/{paper_id}")
            if status == 200:
                paper_data = data["data"]
                requests_data.append({
                    'request_num': i + 1,
                    'paper_id': paper_data.get("paperId"),
                    'title': paper_data.get("title"),
                    'year': paper_data.get("year"),
                    'citation_count': paper_data.get("citationCount")
                })
            await asyncio.sleep(0.5)
        
        # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        if len(requests_data) >= 2:
            first_request = requests_data[0]
            consistency_check = all(
                req['paper_id'] == first_request['paper_id'] and
                req['title'] == first_request['title'] and
                req['year'] == first_request['year']
                for req in requests_data[1:]
            )
        else:
            consistency_check = False
        
        results['multiple_requests_consistency'] = {
            'requests_count': len(requests_data),
            'data_consistent': consistency_check,
            'requests_data': requests_data
        }
        
        # 2. æœç´¢ç»“æœä¸€è‡´æ€§
        query = "attention is all you need"
        search_results = []
        
        for i in range(2):
            status, data, _ = await self._make_request(
                "GET", "/paper/search",
                params={"query": query, "limit": 5}
            )
            if status == 200:
                papers = data["data"]["papers"]
                search_results.append({
                    'request_num': i + 1,
                    'results_count': len(papers),
                    'first_paper_id': papers[0]["paperId"] if papers else None,
                    'paper_ids': [p["paperId"] for p in papers]
                })
            await asyncio.sleep(1)
        
        search_consistency = False
        if len(search_results) >= 2:
            search_consistency = (
                search_results[0]['paper_ids'] == search_results[1]['paper_ids']
            )
        
        results['search_consistency'] = {
            'query': query,
            'searches_count': len(search_results),
            'results_consistent': search_consistency,
            'search_data': search_results
        }
        
        return results
    
    # ========== ä¸»æµ‹è¯•æµç¨‹ ==========
    
    async def run_comprehensive_tests(self):
        """è¿è¡Œå…¨é¢çš„ç«¯åˆ°ç«¯æµ‹è¯•"""
        print("ğŸš€ å¯åŠ¨Paper Parserå…¨é¢ç«¯åˆ°ç«¯æµ‹è¯•")
        print("=" * 60)
        
        # å®šä¹‰æµ‹è¯•ç”¨ä¾‹
        test_suites = [
            ("ç³»ç»Ÿå¥åº·æ£€æŸ¥", self.test_system_health),
            ("è®ºæ–‡æœç´¢å…¨é¢æµ‹è¯•", self.test_paper_search_comprehensive),
            ("è®ºæ–‡è¯¦æƒ…å…¨é¢æµ‹è¯•", self.test_paper_detail_comprehensive),
            ("è®ºæ–‡å…³ç³»å…¨é¢æµ‹è¯•", self.test_paper_relations_comprehensive),
            ("æ‰¹é‡æ“ä½œæµ‹è¯•", self.test_batch_operations),
            ("é”™è¯¯å¤„ç†å…¨é¢æµ‹è¯•", self.test_error_handling_comprehensive),
            ("æ€§èƒ½å…¨é¢æµ‹è¯•", self.test_performance_comprehensive),
            ("æ•°æ®ä¸€è‡´æ€§æµ‹è¯•", self.test_data_consistency),
        ]
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        for test_name, test_func in test_suites:
            await self._run_test(test_name, test_func)
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        await self._generate_comprehensive_report()
    
    async def _generate_comprehensive_report(self):
        """ç”Ÿæˆå…¨é¢çš„æµ‹è¯•æŠ¥å‘Š"""
        total = len(self.metrics)
        passed = len([m for m in self.metrics if m.status == "PASS"])
        failed = len([m for m in self.metrics if m.status == "FAIL"])
        warned = len([m for m in self.metrics if m.status == "WARN"])
        
        success_rate = (passed / total * 100) if total > 0 else 0
        total_duration = sum(m.duration for m in self.metrics)
        avg_response_time = sum(m.response_time for m in self.metrics if m.response_time > 0) / max(1, len([m for m in self.metrics if m.response_time > 0]))
        
        print("\n" + "=" * 60)
        print("ğŸ“Š Paper Parser å…¨é¢ç«¯åˆ°ç«¯æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        print(f"æµ‹è¯•æ€»æ•°: {total}")
        print(f"âœ… é€šè¿‡: {passed}")
        print(f"âŒ å¤±è´¥: {failed}")
        print(f"âš ï¸  è­¦å‘Š: {warned}")
        print(f"ğŸ¯ æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        print(f"ğŸ“ˆ å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}ç§’")
        print("=" * 60)
        
        # è¯¦ç»†ç»“æœ
        for metric in self.metrics:
            status_emoji = {"PASS": "âœ…", "FAIL": "âŒ", "WARN": "âš ï¸", "SKIP": "â­ï¸"}
            print(f"{status_emoji[metric.status]} {metric.test_name}")
            print(f"   â±ï¸  è€—æ—¶: {metric.duration:.2f}s")
            if metric.response_time > 0:
                print(f"   ğŸŒ å“åº”æ—¶é—´: {metric.response_time:.3f}s")
            if metric.cache_hit:
                print(f"   ğŸ“¦ ç¼“å­˜: {metric.cache_hit}")
            if metric.error:
                print(f"   âŒ é”™è¯¯: {metric.error}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_data = {
            "test_info": {
                "test_suite": "Paper Parser E2E Comprehensive Tests",
                "timestamp": datetime.now().isoformat(),
                "base_url": self.base_url,
                "total_tests": total,
                "duration_seconds": total_duration
            },
            "summary": {
                "passed": passed,
                "failed": failed,
                "warned": warned,
                "success_rate": success_rate,
                "avg_response_time": avg_response_time
            },
            "detailed_results": [asdict(metric) for metric in self.metrics]
        }
        
        report_file = f"e2e_comprehensive_report_{timestamp}.json"
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è¯¦ç»†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # æ€§èƒ½æ‘˜è¦
        response_times = [m.response_time for m in self.metrics if m.response_time > 0]
        if response_times:
            print(f"\nğŸ“ˆ æ€§èƒ½æ‘˜è¦:")
            print(f"   æœ€å¿«å“åº”: {min(response_times):.3f}s")
            print(f"   æœ€æ…¢å“åº”: {max(response_times):.3f}s")
            print(f"   å¹³å‡å“åº”: {sum(response_times)/len(response_times):.3f}s")
        
        # å»ºè®®
        if failed > 0:
            print(f"\nâš ï¸  å‘ç° {failed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€å’Œé…ç½®")
        if success_rate < 80:
            print(f"\nâš ï¸  æµ‹è¯•æˆåŠŸç‡è¾ƒä½ ({success_rate:.1f}%)ï¼Œå»ºè®®è¿›è¡Œç³»ç»Ÿè¯Šæ–­")
        elif success_rate >= 95:
            print(f"\nğŸ‰ æµ‹è¯•æˆåŠŸç‡ä¼˜ç§€ ({success_rate:.1f}%)ï¼Œç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼")


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Paper Parser å…¨é¢ç«¯åˆ°ç«¯æµ‹è¯•")
    parser.add_argument("--url", default="http://127.0.0.1:8000", help="APIæœåŠ¡åœ°å€")
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    if args.verbose:
        print(f"ğŸ”— æµ‹è¯•ç›®æ ‡: {args.url}")
        print(f"ğŸ• å¼€å§‹æ—¶é—´: {datetime.now().isoformat()}")
    
    try:
        async with ComprehensiveE2ETester(args.url) as tester:
            await tester.run_comprehensive_tests()
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
