# Paper Parser ç³»ç»Ÿæ¶æ„è®¾è®¡æ–‡æ¡£ v1.0

## ğŸ“‹ æ¦‚è¿°

Paper Parser æ˜¯ä¸€ä¸ªåŸºäº Semantic Scholar API çš„å­¦æœ¯è®ºæ–‡ç¼“å­˜å’Œä»£ç†æœåŠ¡ï¼Œæ—¨åœ¨æä¾›å¿«é€Ÿã€ç¨³å®šçš„è®ºæ–‡æ•°æ®è®¿é—®èƒ½åŠ›ã€‚ç³»ç»Ÿé‡‡ç”¨"æ ¸å¿ƒç¼“å­˜ + å…¶ä»–è½¬å‘"çš„ç­–ç•¥ï¼Œå¯¹çƒ­é—¨ API è¿›è¡Œæ·±åº¦ä¼˜åŒ–ï¼Œå…¶ä»– API ç›´æ¥ä»£ç†è½¬å‘ã€‚

### ğŸ¯ è®¾è®¡ç›®æ ‡

- **é«˜æ€§èƒ½**ï¼šä¸‰çº§ç¼“å­˜æ¶æ„ï¼Œæ¯«ç§’çº§å“åº”çƒ­é—¨æ•°æ®
- **é«˜å¯ç”¨**ï¼šå¼‚æ­¥å¤„ç†ï¼Œä¸é˜»å¡ç”¨æˆ·è¯·æ±‚
- **æ˜“ç»´æŠ¤**ï¼šåˆ†å±‚æ¶æ„ï¼ŒèŒè´£æ¸…æ™°
- **æ¸è¿›å¼**ï¼šæ ¸å¿ƒåŠŸèƒ½å…ˆè¡Œï¼Œé€æ­¥æ‰©å±•
- **å®Œå…¨å…¼å®¹**ï¼šå¯¹å¤– API å®Œå…¨å…¼å®¹ Semantic Scholar

### ğŸ—ï¸ æŠ€æœ¯æ ˆ

- **API å±‚**ï¼šFastAPI + Uvicorn
- **ç¼“å­˜å±‚**ï¼šRedis (çƒ­æ•°æ®ç¼“å­˜ + ä»»åŠ¡çŠ¶æ€)
- **å­˜å‚¨å±‚**ï¼šNeo4j (ç»“æ„åŒ–å­˜å‚¨ + å…³ç³»æŸ¥è¯¢)
- **ä»»åŠ¡é˜Ÿåˆ—**ï¼šARQ + Redis
- **å¤–éƒ¨ API**ï¼šSemantic Scholar API
- **ç›‘æ§**ï¼šPrometheus + Grafana
- **æ—¥å¿—**ï¼šLoguru + ELK Stack

## ğŸ›ï¸ ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    1. API Gateway Layer                 â”‚
â”‚                   (FastAPI Router)                     â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  Core APIs      â”‚  Proxy APIs     â”‚  Health Check   â”‚â”‚
â”‚  â”‚  (ç¼“å­˜+è§£æ)    â”‚  (ç›´æ¥è½¬å‘)     â”‚                 â”‚â”‚
â”‚  â”‚                 â”‚                 â”‚                 â”‚â”‚
â”‚  â”‚ /paper/{id}     â”‚ /author/{id}    â”‚ /health         â”‚â”‚
â”‚  â”‚ /paper/search   â”‚ /paper/bulk     â”‚ /metrics        â”‚â”‚
â”‚  â”‚ /paper/batch    â”‚ /autocomplete   â”‚                 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  2. Service Layer                      â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ CorePaperServiceâ”‚  ProxyService   â”‚  TaskService    â”‚â”‚
â”‚  â”‚                 â”‚                 â”‚                 â”‚â”‚
â”‚  â”‚ - ä¸‰çº§ç¼“å­˜é€»è¾‘  â”‚ - S2 APIè½¬å‘    â”‚ - å¼‚æ­¥ä»»åŠ¡ç®¡ç†  â”‚â”‚
â”‚  â”‚ - æ•°æ®è§£æå…¥åº“  â”‚ - è¯·æ±‚/å“åº”åŒ…è£… â”‚ - çŠ¶æ€è¿½è¸ª      â”‚â”‚
â”‚  â”‚ - çŠ¶æ€ç®¡ç†      â”‚ - é”™è¯¯å¤„ç†      â”‚                 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                3. Data Access Layer                    â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   RedisClient   â”‚   Neo4jClient   â”‚   S2Client      â”‚â”‚
â”‚  â”‚                 â”‚                 â”‚                 â”‚â”‚
â”‚  â”‚ - JSONç¼“å­˜      â”‚ - ç»“æ„åŒ–å­˜å‚¨    â”‚ - HTTPå®¢æˆ·ç«¯    â”‚â”‚
â”‚  â”‚ - ä»»åŠ¡çŠ¶æ€      â”‚ - å…³ç³»æŸ¥è¯¢      â”‚ - é™æµé‡è¯•      â”‚â”‚
â”‚  â”‚ - ä¼šè¯ç®¡ç†      â”‚ - æ‰¹é‡æ“ä½œ      â”‚ - é”™è¯¯å¤„ç†      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚               4. Background Task Layer                 â”‚
â”‚                  (ARQ Workers)                         â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ DataIngestion   â”‚ CacheManagement â”‚ SystemMaintain  â”‚â”‚
â”‚  â”‚                 â”‚                 â”‚                 â”‚â”‚
â”‚  â”‚ - è§£æS2æ•°æ®    â”‚ - ç¼“å­˜é¢„çƒ­      â”‚ - æ¸…ç†è¿‡æœŸæ•°æ®  â”‚â”‚
â”‚  â”‚ - å…¥åº“Neo4j     â”‚ - ç¼“å­˜æ›´æ–°      â”‚ - å¥åº·æ£€æŸ¥      â”‚â”‚
â”‚  â”‚ - çŠ¶æ€æ›´æ–°      â”‚ - å¤±æ•ˆå¤„ç†      â”‚ - æ—¥å¿—å½’æ¡£      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ æ•°æ®æµè®¾è®¡

### æ ¸å¿ƒ API æ•°æ®æµ

```mermaid
sequenceDiagram
    participant Client
    participant API Gateway
    participant CoreService
    participant Redis
    participant Neo4j
    participant S2 API
    participant ARQ

    Client->>API Gateway: GET /paper/{id}
    API Gateway->>CoreService: get_paper(id)
    
    CoreService->>Redis: æŸ¥è¯¢ç¼“å­˜
    alt ç¼“å­˜å‘½ä¸­
        Redis-->>CoreService: è¿”å›JSONæ•°æ®
        CoreService-->>API Gateway: è¿”å›ç»“æœ
        API Gateway-->>Client: 200 OK + æ•°æ®
    else ç¼“å­˜æœªå‘½ä¸­
        CoreService->>Neo4j: æŸ¥è¯¢æŒä¹…åŒ–æ•°æ®
        alt Neo4jæœ‰æ•°æ®ä¸”æ–°é²œ
            Neo4j-->>CoreService: è¿”å›æ•°æ®
            CoreService->>ARQ: å¼‚æ­¥æ›´æ–°Redis
            CoreService-->>API Gateway: è¿”å›ç»“æœ
            API Gateway-->>Client: 200 OK + æ•°æ®
        else éœ€è¦ä»S2è·å–
            CoreService->>S2 API: è°ƒç”¨åŸå§‹API
            S2 API-->>CoreService: è¿”å›åŸå§‹æ•°æ®
            CoreService->>Redis: ç«‹å³ç¼“å­˜
            CoreService->>ARQ: å¼‚æ­¥å…¥åº“Neo4j
            CoreService-->>API Gateway: è¿”å›ç»“æœ
            API Gateway-->>Client: 200 OK + æ•°æ®
        end
    end
```

### ä»£ç† API æ•°æ®æµ

```mermaid
sequenceDiagram
    participant Client
    participant API Gateway
    participant ProxyService
    participant S2 API

    Client->>API Gateway: GET /author/{id}
    API Gateway->>ProxyService: proxy_request()
    ProxyService->>S2 API: ç›´æ¥è½¬å‘è¯·æ±‚
    S2 API-->>ProxyService: è¿”å›åŸå§‹å“åº”
    ProxyService-->>API Gateway: è½¬å‘å“åº”
    API Gateway-->>Client: åŸå§‹å“åº”
```

## ğŸ“Š æ•°æ®æ¨¡å‹è®¾è®¡

### Redis å­˜å‚¨ç»“æ„

```python
# çƒ­æ•°æ®ç¼“å­˜ (TTL: 1å°æ—¶)
paper:paperId:full â†’ å®Œæ•´JSONæ•°æ®
paper:paperId:basic â†’ åŸºç¡€ä¿¡æ¯JSON  
paper:doi:10.xxx â†’ paperIdæ˜ å°„
paper:arxiv:1234.5678 â†’ paperIdæ˜ å°„

# ä»»åŠ¡çŠ¶æ€ç¼“å­˜ (TTL: 10åˆ†é’Ÿ)  
task:paperId:status â†’ {"status": "processing|completed|failed", "progress": 80}

# æœç´¢ç»“æœç¼“å­˜ (TTL: 30åˆ†é’Ÿ)
search:query_hash â†’ {"results": [...], "total": 1000, "cached_at": "..."}

# ç³»ç»ŸçŠ¶æ€ç¼“å­˜
system:s2_api_status â†’ {"available": true, "last_check": "...", "rate_limit": {...}}
```

### Neo4j æ•°æ®æ¨¡å‹ v1.0

```cypher
// è®ºæ–‡èŠ‚ç‚¹ - ä¼˜åŒ–çš„å­˜å‚¨ç­–ç•¥
CREATE (p:Paper {
  paperId: "string",           // S2ä¸»é”®
  corpusId: 123,               // S2æ¬¡é”®  
  title: "string",             // æ ‡é¢˜
  abstract: "string",          // æ‘˜è¦
  year: 2023,                  // å¹´ä»½
  citationCount: 100,          // å¼•ç”¨æ•°
  referenceCount: 50,          // å‚è€ƒæ–‡çŒ®æ•°
  influentialCitationCount: 10,// æœ‰å½±å“åŠ›å¼•ç”¨æ•°
  venue: "string",             // å‘è¡¨åœºæ‰€
  fieldsOfStudy: ["CS"],       // ç ”ç©¶é¢†åŸŸ
  isOpenAccess: true,          // å¼€æ”¾è·å–
  
  // v1.0 æ–°å¢ï¼šå†…è”å­˜å‚¨å°æ•°æ®
  metadataJson: "å®Œæ•´metadata JSONå­—ç¬¦ä¸²",  // è®ºæ–‡å…ƒæ•°æ®
  metadataUpdated: datetime(), // å…ƒæ•°æ®æ›´æ–°æ—¶é—´
  
  // v1.0 æ–°å¢ï¼šå¤–éƒ¨IDå­˜å‚¨
  externalIds: "{\"DOI\":\"10.1234/example\",\"ArXiv\":\"2106.15928\"}", // å¤–éƒ¨ID JSON
  title_norm: "normalized title",  // æ ‡é¢˜å½’ä¸€åŒ–ï¼ˆç”¨äºæœç´¢ï¼‰
  
  dataJson: "å®Œæ•´JSONå­—ç¬¦ä¸²",   // åŸå§‹æ•°æ®å¤‡ä»½
  lastUpdated: datetime(),     // æœ€åæ›´æ–°æ—¶é—´
  ingestStatus: "full",        // "stub" | "full"
  source: "s2"                 // æ•°æ®æº
})

// å¤–éƒ¨IDæ˜ å°„ - å­˜å‚¨ä¸ºPaperèŠ‚ç‚¹JSONå±æ€§
// å¤–éƒ¨IDç›´æ¥å­˜å‚¨åœ¨PaperèŠ‚ç‚¹çš„externalIds JSONå­—æ®µä¸­
// æ ¼å¼ï¼š{"DOI": "10.1234/example", "ArXiv": "2106.15928", "CorpusId": "123456"}
// æ”¯æŒçš„IDç±»å‹ï¼šDOI, ArXiv, CorpusId, URL, MAG, ACL, PMID, PMCID
// ç‰¹æ®Šå¤„ç†ï¼šTITLE_NORM å­˜å‚¨åœ¨å•ç‹¬çš„ title_norm å±æ€§ä¸­

// DataChunk - ç”¨äºå¤§æ•°æ®å­˜å‚¨
CREATE (d:DataChunk {
  paperId: "string",           // å…³è”çš„è®ºæ–‡ID
  chunkType: "citations|references", // æ•°æ®å—ç±»å‹
  dataJson: "å¤§å‹JSONæ•°æ®",    // å¼•ç”¨/å‚è€ƒæ–‡çŒ®æ•°æ®
  lastUpdated: datetime(),     // æœ€åæ›´æ–°æ—¶é—´
  recordCount: 100             // è®°å½•æ•°é‡
})

// ä½œè€…èŠ‚ç‚¹ (ç®€åŒ–ç‰ˆ)
CREATE (a:Author {
  authorId: "string",
  name: "string",
  paperCount: 100,
  citationCount: 1000
})

// å…³ç³»å®šä¹‰
CREATE (p1:Paper)-[:CITES]->(p2:Paper)      // å¼•ç”¨å…³ç³»
CREATE (p:Paper)-[:AUTHORED_BY]->(a:Author) // ä½œè€…å…³ç³»
CREATE (p:Paper)-[:HAS_CITATIONS]->(d:DataChunk {chunkType: 'citations'})
CREATE (p:Paper)-[:HAS_REFERENCES]->(d:DataChunk {chunkType: 'references'})
```

### æ•°æ®å­˜å‚¨ç­–ç•¥ä¼˜åŒ–

**v1.0 å…³é”®æ”¹è¿›ï¼š**

1. **å°æ•°æ®å†…è”å­˜å‚¨**ï¼š
   - `Paper.metadataJson` ç›´æ¥å­˜å‚¨å…ƒæ•°æ®ï¼Œé¿å…é¢å¤–èŠ‚ç‚¹
   - å‡å°‘æŸ¥è¯¢å¤æ‚åº¦å’Œç½‘ç»œå¾€è¿”
   - æé«˜å°æ•°æ®è®¿é—®æ€§èƒ½

2. **å¤§æ•°æ®åˆ†å—å­˜å‚¨**ï¼š
   - `DataChunk` èŠ‚ç‚¹å­˜å‚¨ Citations/References å¤§æ•°æ®
   - æ”¯æŒæ¸è¿›å¼ç»“æ„åŒ–è½¬æ¢
   - ä¾¿äºæ‰¹å¤„ç†å’Œç‰ˆæœ¬ç®¡ç†

3. **ç»Ÿä¸€çš„å¤–éƒ¨IDç³»ç»Ÿ**ï¼š
   - å¤–éƒ¨IDå­˜å‚¨åœ¨PaperèŠ‚ç‚¹çš„JSONå±æ€§ä¸­
   - æ ‡å‡†åŒ–çš„å½’ä¸€åŒ–è§„åˆ™
   - æ”¯æŒå¤šç§IDç±»å‹ï¼ˆDOI, ArXiv, CorpusIdç­‰ï¼‰

### ç´¢å¼•ç­–ç•¥

```cypher
// æ ¸å¿ƒç´¢å¼•
CREATE INDEX paper_id FOR (p:Paper) ON (p.paperId)
CREATE INDEX corpus_id FOR (p:Paper) ON (p.corpusId)
CREATE INDEX paper_title FOR (p:Paper) ON (p.title)
CREATE INDEX paper_year FOR (p:Paper) ON (p.year)

// å¤–éƒ¨IDç´¢å¼• - åŸºäºJSONå±æ€§æŸ¥è¯¢ï¼Œéœ€è¦APOCæ”¯æŒ
// å®é™…æŸ¥è¯¢ä½¿ç”¨ï¼šWHERE apoc.convert.fromJsonMap(p.externalIds)['DOI'] = $value
// æ ‡é¢˜å½’ä¸€åŒ–ç´¢å¼•
CREATE INDEX paper_title_norm FOR (p:Paper) ON (p.title_norm)

// DataChunk ç´¢å¼•
CREATE INDEX chunk_paper_type FOR (d:DataChunk) ON (d.paperId, d.chunkType)

// ä½œè€…ç´¢å¼•
CREATE INDEX author_id FOR (a:Author) ON (a.authorId)
CREATE INDEX author_name FOR (a:Author) ON (a.name)

// å¤åˆç´¢å¼•
CREATE INDEX paper_year_citations FOR (p:Paper) ON (p.year, p.citationCount)
```

## ğŸ¯ API åˆ†ç±»ç­–ç•¥

### æ ¸å¿ƒ APIs (å®Œæ•´ç¼“å­˜ç­–ç•¥)

è¿™äº› API å®ç°ä¸‰çº§ç¼“å­˜ + å¼‚æ­¥å¤„ç†ï¼š

```python
GET  /paper/{paper_id}                    # æ–‡çŒ®è¯¦æƒ… [æœ€é«˜ä¼˜å…ˆçº§]
GET  /paper/{paper_id}/citations          # å¼•ç”¨æ–‡çŒ® [é«˜ä¼˜å…ˆçº§]  
GET  /paper/{paper_id}/references         # å‚è€ƒæ–‡çŒ® [é«˜ä¼˜å…ˆçº§]
GET  /paper/search                        # æ–‡çŒ®æœç´¢ [é«˜ä¼˜å…ˆçº§]
POST /paper/batch                         # æ‰¹é‡æŸ¥è¯¢ [ä¸­ä¼˜å…ˆçº§]
```

### ä»£ç† APIs (ç›´æ¥è½¬å‘)

è¿™äº› API ç›´æ¥ä»£ç†åˆ° S2ï¼Œä¸åšç¼“å­˜ï¼š

```python
GET  /paper/{paper_id}/authors            # ä½œè€…ä¿¡æ¯
GET  /author/{author_id}                  # ä½œè€…è¯¦æƒ…
GET  /author/{author_id}/papers           # ä½œè€…è®ºæ–‡
GET  /paper/search/match                  # ç²¾ç¡®åŒ¹é…
GET  /paper/search/bulk                   # æ‰¹é‡æœç´¢
GET  /paper/autocomplete                  # è‡ªåŠ¨è¡¥å…¨
# ... å…¶ä»–æ‰€æœ‰S2 API
```

## âš™ï¸ æ ¸å¿ƒæœåŠ¡è®¾è®¡

### CorePaperService v1.0

```python
class CorePaperService:
    """æ ¸å¿ƒè®ºæ–‡æœåŠ¡ - å®ç°ä¸‰çº§ç¼“å­˜ç­–ç•¥ v1.0"""
    
    def __init__(self, redis_client, neo4j_client, s2_client, task_queue):
        self.redis = redis_client
        self.neo4j = neo4j_client 
        self.s2 = s2_client
        self.tasks = task_queue
    
    async def get_paper(self, paper_id: str, fields: str = None) -> dict:
        """è·å–è®ºæ–‡ä¿¡æ¯ - ä¸‰çº§ç¼“å­˜ç­–ç•¥"""
        
        # 1. Redisç¼“å­˜æŸ¥è¯¢ (æ¯«ç§’çº§)
        cache_key = f"paper:{paper_id}:{fields or 'full'}"
        cached = await self.redis.get(cache_key)
        if cached:
            return json.loads(cached)
            
        # 2. Neo4jæŒä¹…åŒ–æŸ¥è¯¢ (10msçº§) - ä½¿ç”¨å†…è”metadata
        neo4j_data = await self.neo4j.get_paper(paper_id)
        if neo4j_data and self._is_data_fresh(neo4j_data):
            # å¼‚æ­¥æ›´æ–°Redis
            asyncio.create_task(self._update_cache(cache_key, neo4j_data))
            return neo4j_data
            
        # 3. æ£€æŸ¥å¤„ç†çŠ¶æ€
        task_status = await self.redis.get(f"task:{paper_id}:status")
        if task_status == "processing":
            # ç­‰å¾…æœ€å¤š3ç§’
            for i in range(6):
                await asyncio.sleep(0.5)
                cached = await self.redis.get(cache_key)
                if cached:
                    return json.loads(cached)
            # è¶…æ—¶å¤„ç†
            raise HTTPException(408, "Request timeout, please try again")
        
        # 4. è°ƒç”¨S2 API (åŒæ­¥ç­‰å¾…)
        await self.redis.set(f"task:{paper_id}:status", "processing", ex=300)
        try:
            s2_data = await self.s2.get_paper(paper_id, fields)
            
            # ç«‹å³ç¼“å­˜å¹¶è¿”å›
            await self.redis.setex(cache_key, 3600, json.dumps(s2_data))
            
            # å¼‚æ­¥å…¥åº“Neo4j - ä½¿ç”¨æ–°çš„å­˜å‚¨ç­–ç•¥
            self.tasks.ingest_paper_data_v1.delay(s2_data)
            
            await self.redis.delete(f"task:{paper_id}:status")
            return s2_data
            
        except Exception as e:
            await self.redis.set(f"task:{paper_id}:status", "failed", ex=60)
            raise HTTPException(500, f"Failed to fetch paper: {str(e)}")
    
    def _is_data_fresh(self, data: dict, max_age_hours: int = 24) -> bool:
        """æ£€æŸ¥æ•°æ®æ˜¯å¦æ–°é²œ"""
        last_updated = data.get('metadataUpdated') or data.get('lastUpdated')
        if not last_updated:
            return False
        age = datetime.now() - datetime.fromisoformat(last_updated)
        return age.total_seconds() < max_age_hours * 3600
```

### Neo4jClient v1.0 ä¼˜åŒ–

```python
class Neo4jClient:
    """Neo4jå®¢æˆ·ç«¯ v1.0 - ä¼˜åŒ–çš„æ•°æ®å­˜å‚¨"""
    
    async def merge_paper(self, paper_data: dict) -> bool:
        """åˆå¹¶è®ºæ–‡æ•°æ® - ä½¿ç”¨å†…è”metadataå’Œå¤–éƒ¨IDå­˜å‚¨"""
        query = """
        MERGE (p:Paper {paperId: $paperId})
        SET p.title = $title,
            p.abstract = $abstract,
            p.year = $year,
            p.citationCount = $citationCount,
            p.referenceCount = $referenceCount,
            p.metadataJson = $metadataJson,
            p.metadataUpdated = datetime(),
            p.externalIds = $externalIds,
            p.dataJson = $dataJson,
            p.lastUpdated = datetime(),
            p.ingestStatus = 'full'
        RETURN p.paperId
        """
        
        # æå–metadataä¸ºJSONå­—ç¬¦ä¸²
        metadata = {
            'title': paper_data.get('title'),
            'abstract': paper_data.get('abstract'),
            'year': paper_data.get('year'),
            'venue': paper_data.get('venue'),
            'fieldsOfStudy': paper_data.get('fieldsOfStudy', []),
            # ... å…¶ä»–metadataå­—æ®µ
        }
        
        # å¤„ç†å¤–éƒ¨ID
        external_ids = paper_data.get('externalIds', {})
        if paper_data.get('corpusId'):
            external_ids['CorpusId'] = paper_data['corpusId']
        
        params = {
            'paperId': paper_data['paperId'],
            'title': paper_data.get('title'),
            'abstract': paper_data.get('abstract'),
            'year': paper_data.get('year'),
            'citationCount': paper_data.get('citationCount', 0),
            'referenceCount': paper_data.get('referenceCount', 0),
            'metadataJson': json.dumps(metadata),
            'externalIds': json.dumps(external_ids) if external_ids else None,
            'dataJson': json.dumps(paper_data)
        }
        
        result = await self.execute_query(query, params)
        return bool(result)
    
    async def merge_data_chunks(self, paper_id: str, citations: list, references: list) -> bool:
        """åˆå¹¶å¤§æ•°æ®å—"""
        queries = []
        
        if citations:
            queries.append({
                'query': """
                MERGE (p:Paper {paperId: $paperId})
                MERGE (d:DataChunk {paperId: $paperId, chunkType: 'citations'})
                SET d.dataJson = $dataJson,
                    d.lastUpdated = datetime(),
                    d.recordCount = $recordCount
                MERGE (p)-[:HAS_CITATIONS]->(d)
                """,
                'params': {
                    'paperId': paper_id,
                    'dataJson': json.dumps(citations),
                    'recordCount': len(citations)
                }
            })
        
        if references:
            queries.append({
                'query': """
                MERGE (p:Paper {paperId: $paperId})
                MERGE (d:DataChunk {paperId: $paperId, chunkType: 'references'})
                SET d.dataJson = $dataJson,
                    d.lastUpdated = datetime(),
                    d.recordCount = $recordCount
                MERGE (p)-[:HAS_REFERENCES]->(d)
                """,
                'params': {
                    'paperId': paper_id,
                    'dataJson': json.dumps(references),
                    'recordCount': len(references)
                }
            })
        
        # æ‰¹é‡æ‰§è¡Œ
        for query_data in queries:
            await self.execute_query(query_data['query'], query_data['params'])
        
        return True
    
    async def get_paper(self, paper_id: str) -> dict:
        """è·å–è®ºæ–‡åŠå…¶å†…è”metadataå’Œå¤–éƒ¨ID"""
        query = """
        MATCH (p:Paper {paperId: $paperId})
        RETURN p.paperId, p.metadataJson, p.metadataUpdated, p.externalIds, p.dataJson, p.lastUpdated
        """
        
        result = await self.execute_query(query, {'paperId': paper_id})
        if not result:
            return None
        
        record = result[0]
        
        # ä¼˜å…ˆä½¿ç”¨å®Œæ•´dataJsonï¼ˆåŒ…å«æ‰€æœ‰æ•°æ®ï¼‰
        if record['p.dataJson']:
            data = json.loads(record['p.dataJson'])
            data['_cached_at'] = record['p.lastUpdated']
            
            # å¦‚æœæœ‰æ›´æ–°çš„metadataJsonï¼Œåˆå¹¶è¿›å»
            if record['p.metadataJson'] and record['p.metadataUpdated']:
                metadata = json.loads(record['p.metadataJson'])
                data.update(metadata)
                data['_metadata_updated'] = record['p.metadataUpdated']
            
            return data
        
        # å¦‚æœåªæœ‰metadataJson
        if record['p.metadataJson']:
            metadata = json.loads(record['p.metadataJson'])
            metadata['_cached_at'] = record['p.metadataUpdated']
            return metadata
        
        return None
```

## ğŸ”§ å¼‚æ­¥ä»»åŠ¡è®¾è®¡ v1.0

### ä»»åŠ¡åˆ†ç±»å’Œä¼˜å…ˆçº§

```python
# ARQ å¼‚æ­¥ä»»åŠ¡å®šä¹‰ v1.0
async def ingest_paper_data_v1(paper_data: dict):
    """v1.0 ä¼˜åŒ–çš„è®ºæ–‡æ•°æ®å…¥åº“æµç¨‹"""
    paper_id = paper_data['paperId']
    
    try:
        # 1. å­˜å‚¨åŸºæœ¬è®ºæ–‡ä¿¡æ¯å’Œmetadata
        await neo4j_client.merge_paper(paper_data)
        
        # 2. å¤„ç†å¤–éƒ¨IDæ˜ å°„ - å­˜å‚¨åœ¨externalIds JSONå±æ€§ä¸­
        # è¿™ä¸ªæ­¥éª¤åœ¨merge_paperä¸­å·²ç»å¤„ç†
        
        # 3. å¤„ç†å¤§æ•°æ®å— (citations/references)
        citations = paper_data.get('citations', [])
        references = paper_data.get('references', [])
        
        if citations or references:
            await neo4j_client.merge_data_chunks_from_full_data(paper_data)
        
        # 4. æ ¹æ®æ•°æ®å¤§å°å†³å®šæ˜¯å¦ç«‹å³åˆ›å»ºå…³ç³»
        citation_count = len(citations)
        reference_count = len(references)
        
        # å°è§„æ¨¡æ•°æ®ï¼šç«‹å³åˆ›å»ºå…³ç³»
        if citation_count <= 100 and reference_count <= 100:
            await neo4j_client.create_citation_relationships(paper_id, citations, references)
        else:
            # å¤§è§„æ¨¡æ•°æ®ï¼šåˆ›å»ºå¤„ç†è®¡åˆ’
            await task_queue.enqueue_job('create_relationships_batch', paper_id)
        
        logger.info(f"Successfully ingested paper {paper_id} with {citation_count} citations, {reference_count} references")
        
    except Exception as e:
        logger.error(f"Failed to ingest paper {paper_id}: {str(e)}")
        raise

async def create_relationships_batch(paper_id: str):
    """æ‰¹é‡åˆ›å»ºå¼•ç”¨å…³ç³» - å¤§æ•°æ®å¤„ç†"""
    try:
        # ä»DataChunkè¯»å–æ•°æ®
        chunks = await neo4j_client.get_data_chunks(paper_id)
        
        for chunk in chunks:
            if chunk['chunkType'] == 'citations':
                citations = json.loads(chunk['dataJson'])
                await neo4j_client.create_citation_relationships_batch(paper_id, citations, 'citations')
            elif chunk['chunkType'] == 'references':
                references = json.loads(chunk['dataJson'])
                await neo4j_client.create_citation_relationships_batch(paper_id, references, 'references')
        
        logger.info(f"Successfully created relationships for paper {paper_id}")
        
    except Exception as e:
        logger.error(f"Failed to create relationships for paper {paper_id}: {str(e)}")
        raise
```

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ v1.0

### å“åº”æ—¶é—´ç›®æ ‡

```
ç¼“å­˜å‘½ä¸­ (Redis):     < 5ms    (ä¼˜åŒ–å)
æŒä¹…åŒ–å‘½ä¸­ (Neo4j):   < 20ms   (å†…è”metadataä¼˜åŒ–)
S2 APIè°ƒç”¨:           < 2000ms (è¿æ¥æ± ä¼˜åŒ–)
æ‰¹é‡æŸ¥è¯¢ (10ç¯‡):      < 200ms  (æ‰¹é‡ä¼˜åŒ–)
æœç´¢æŸ¥è¯¢:             < 100ms  (ç´¢å¼•ä¼˜åŒ–)
```

### ç¼“å­˜å‘½ä¸­ç‡ç›®æ ‡

```
çƒ­é—¨è®ºæ–‡ (Top 1000):   > 98%   (é¢„çƒ­ç­–ç•¥)
ä¸€èˆ¬è®ºæ–‡:              > 80%   (æ™ºèƒ½ç¼“å­˜)
æœç´¢ç»“æœ:              > 70%   (æŸ¥è¯¢ä¼˜åŒ–)
æ‰¹é‡æŸ¥è¯¢:              > 85%   (æ‰¹é‡ç¼“å­˜)
```

### ç³»ç»Ÿå®¹é‡è§„åˆ’

```
Redis å†…å­˜:           16GB (çº¦200ä¸‡ç¯‡è®ºæ–‡ç¼“å­˜)
Neo4j å­˜å‚¨:          500GB (çº¦5000ä¸‡ç¯‡è®ºæ–‡)
ARQ å·¥ä½œè¿›ç¨‹:         3ä¸ª (æ¯ä¸ªæœ€å¤š20ä¸ªå¹¶å‘ä»»åŠ¡)
å¹¶å‘è¿æ¥æ•°:          5000ä¸ª
QPS ç›®æ ‡:            2000 req/s
```

## ğŸ›¡ï¸ é”™è¯¯å¤„ç†ç­–ç•¥ v1.0

### åˆ†çº§é™çº§æœºåˆ¶

```python
# Level 1: Redisä¸å¯ç”¨
if redis_unavailable:
    # ç›´æ¥æŸ¥è¯¢Neo4jï¼Œè·³è¿‡ç¼“å­˜
    return await neo4j_client.get_paper(paper_id)

# Level 2: Neo4jä¸å¯ç”¨  
if neo4j_unavailable:
    # ç›´æ¥è°ƒç”¨S2 APIï¼Œä¸å…¥åº“
    return await s2_client.get_paper(paper_id)

# Level 3: S2 APIä¸å¯ç”¨
if s2_api_unavailable:
    # è¿”å›Neo4jå†å²æ•°æ® + è­¦å‘Š
    data = await neo4j_client.get_paper(paper_id)
    if data:
        data['_warning'] = 'Data may be outdated due to upstream API issues'
        return data
    else:
        raise HTTPException(503, "Service temporarily unavailable")
```

### é™æµå’Œé‡è¯•ç­–ç•¥

```python
# S2 APIé™æµé…ç½® v1.0
S2_RATE_LIMITS = {
    'requests_per_second': 200,  # æå‡é™æµ
    'requests_per_hour': 50000,  # æå‡é™æµ
    'concurrent_requests': 20,   # æå‡å¹¶å‘
    'burst_limit': 50           # æ–°å¢çªå‘é™åˆ¶
}

# é‡è¯•ç­–ç•¥ v1.0
RETRY_CONFIG = {
    'max_attempts': 5,           # å¢åŠ é‡è¯•æ¬¡æ•°
    'backoff_factor': 1.5,       # ä¼˜åŒ–é€€é¿å› å­
    'retry_status_codes': [429, 500, 502, 503, 504],
    'timeout_seconds': 45,       # å¢åŠ è¶…æ—¶æ—¶é—´
    'jitter': True              # æ–°å¢æŠ–åŠ¨
}
```

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿— v1.0

### å…³é”®æŒ‡æ ‡ç›‘æ§

```python
# ä¸šåŠ¡æŒ‡æ ‡ v1.0
- APIå“åº”æ—¶é—´åˆ†å¸ƒ (P50, P90, P95, P99)
- ç¼“å­˜å‘½ä¸­ç‡åˆ†å±‚ç»Ÿè®¡ (Redis, Neo4j, æ€»ä½“)
- S2 APIè°ƒç”¨æˆåŠŸç‡å’Œå»¶è¿Ÿ
- ä»»åŠ¡é˜Ÿåˆ—ç§¯å‹å’Œå¤„ç†æ—¶é—´
- é”™è¯¯ç‡å’Œé”™è¯¯ç±»å‹åˆ†å¸ƒ
- æ•°æ®æ–°é²œåº¦ç»Ÿè®¡

# ç³»ç»ŸæŒ‡æ ‡ v1.0
- CPUã€å†…å­˜ã€ç£ç›˜ä½¿ç”¨ç‡
- Redisè¿æ¥æ± çŠ¶æ€å’Œå†…å­˜ä½¿ç”¨
- Neo4jæŸ¥è¯¢æ€§èƒ½å’Œè¿æ¥æ•°
- ARQå·¥ä½œè¿›ç¨‹çŠ¶æ€å’Œé˜Ÿåˆ—é•¿åº¦
- ç½‘ç»œå»¶è¿Ÿå’Œå¸¦å®½ä½¿ç”¨
- æ•°æ®åº“å­˜å‚¨å¢é•¿è¶‹åŠ¿

# å‘Šè­¦è§„åˆ™ v1.0
- APIé”™è¯¯ç‡ > 3% (é™ä½é˜ˆå€¼)
- å“åº”æ—¶é—´P95 > 500ms (é™ä½é˜ˆå€¼)
- ç¼“å­˜å‘½ä¸­ç‡ < 70% (æé«˜é˜ˆå€¼)
- ä»»åŠ¡é˜Ÿåˆ—ç§¯å‹ > 500 (é™ä½é˜ˆå€¼)
- ç³»ç»Ÿèµ„æºä½¿ç”¨ç‡ > 85%
- æ•°æ®æ–°é²œåº¦ > 48å°æ—¶çš„æ¯”ä¾‹ > 10%
```

### æ—¥å¿—ç­–ç•¥ v1.0

```python
# æ—¥å¿—çº§åˆ«å’Œå†…å®¹ v1.0
INFO:  æ­£å¸¸ä¸šåŠ¡æµç¨‹ (APIè°ƒç”¨ã€ç¼“å­˜å‘½ä¸­ã€æ•°æ®æ›´æ–°ç­‰)
WARN:  å¼‚å¸¸ä½†å¯æ¢å¤ (ç¼“å­˜æœªå‘½ä¸­ã€é‡è¯•ã€é™çº§ç­‰)  
ERROR: é”™è¯¯éœ€è¦å…³æ³¨ (APIå¤±è´¥ã€æ•°æ®åº“é”™è¯¯ã€ä»»åŠ¡å¤±è´¥ç­‰)
DEBUG: è¯¦ç»†è°ƒè¯•ä¿¡æ¯ (ä»…å¼€å‘ç¯å¢ƒ)

# ç»“æ„åŒ–æ—¥å¿—æ ¼å¼ v1.0
{
    "timestamp": "2024-01-01T12:00:00Z",
    "level": "INFO", 
    "service": "core_paper_service",
    "operation": "get_paper",
    "paper_id": "123456",
    "cache_layer": "redis|neo4j|s2api",
    "cache_hit": true,
    "response_time_ms": 15,
    "data_freshness_hours": 2,
    "trace_id": "abc-def-123",
    "user_agent": "client_info",
    "request_fields": "title,abstract,citations"
}
```

## ğŸš€ éƒ¨ç½²æ¶æ„ v1.0

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

```yaml
# docker-compose.yml v1.0
version: '3.8'
services:
  api-gateway:
    image: paper-parser:v1.0
    ports: ["8000:8000"]
    replicas: 5
    resources:
      limits:
        memory: 2G
        cpus: '1'
    
  redis:
    image: redis:7-alpine
    ports: ["6379:6379"]
    volumes: ["redis_data:/data"]
    command: redis-server --maxmemory 16gb --maxmemory-policy allkeys-lru
    
  neo4j:
    image: neo4j:5
    ports: ["7687:7687", "7474:7474"] 
    volumes: ["neo4j_data:/data"]
    environment:
      NEO4J_dbms_memory_heap_initial__size: 4G
      NEO4J_dbms_memory_heap_max__size: 8G
      NEO4J_dbms_memory_pagecache_size: 4G
    
  arq-worker:
    image: paper-parser:v1.0
    command: arq app.tasks.worker.WorkerSettings
    replicas: 3
    resources:
      limits:
        memory: 1G
        cpus: '0.5'

  prometheus:
    image: prom/prometheus
    ports: ["9090:9090"]
    volumes: ["./prometheus.yml:/etc/prometheus/prometheus.yml"]
    
  grafana:
    image: grafana/grafana
    ports: ["3000:3000"]
    volumes: ["grafana_data:/var/lib/grafana"]
```

## ğŸ“… å¼€å‘è®¡åˆ’ v1.0

### Phase 1: æ¶æ„å‡çº§ (Week 1)
- [x] æ•°æ®æ¨¡å‹ä¼˜åŒ– (å†…è”metadata, DataChunkåˆ†ç¦»)
- [x] Neo4jå®¢æˆ·ç«¯é‡æ„
- [ ] CorePaperService v1.0 å®ç°
- [ ] å¼‚æ­¥ä»»åŠ¡ç³»ç»Ÿå‡çº§
- [ ] åŸºç¡€æ€§èƒ½æµ‹è¯•

### Phase 2: æ€§èƒ½ä¼˜åŒ– (Week 2)  
- [ ] ç¼“å­˜ç­–ç•¥ä¼˜åŒ–
- [ ] æ‰¹é‡æ“ä½œä¼˜åŒ–
- [ ] è¿æ¥æ± å’Œèµ„æºç®¡ç†
- [ ] æŸ¥è¯¢æ€§èƒ½è°ƒä¼˜
- [ ] å‹åŠ›æµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•

### Phase 3: ç›‘æ§å®Œå–„ (Week 3)
- [ ] è¯¦ç»†æŒ‡æ ‡æ”¶é›†
- [ ] å‘Šè­¦ç³»ç»Ÿå®Œå–„
- [ ] æ—¥å¿—åˆ†æä¼˜åŒ–
- [ ] æ€§èƒ½ä»ªè¡¨æ¿
- [ ] è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬

### Phase 4: ç”Ÿäº§éƒ¨ç½² (Week 4)
- [ ] ç”Ÿäº§ç¯å¢ƒé…ç½®
- [ ] æ•°æ®è¿ç§»ç­–ç•¥
- [ ] ç°åº¦å‘å¸ƒ
- [ ] ç›‘æ§éªŒè¯
- [ ] æ–‡æ¡£å®Œå–„

## ğŸ”§ é…ç½®ç®¡ç† v1.0

### ç¯å¢ƒå˜é‡é…ç½®

```python
# config/settings.py v1.0
from pydantic import BaseSettings

class Settings(BaseSettings):
    # APIé…ç½®
    API_HOST: str = "0.0.0.0"
    API_PORT: int = 8000
    API_PREFIX: str = "/api/v1"
    API_WORKERS: int = 5
    
    # S2 APIé…ç½®
    S2_API_KEY: str = ""
    S2_BASE_URL: str = "https://api.semanticscholar.org/graph/v1"
    S2_RATE_LIMIT: int = 200  # æå‡é™æµ
    S2_TIMEOUT: int = 45      # å¢åŠ è¶…æ—¶
    S2_MAX_RETRIES: int = 5   # å¢åŠ é‡è¯•
    
    # Redisé…ç½®
    REDIS_URL: str = "redis://localhost:6379/0"
    REDIS_MAX_CONNECTIONS: int = 50    # å¢åŠ è¿æ¥æ± 
    REDIS_DEFAULT_TTL: int = 3600      # 1å°æ—¶
    REDIS_SEARCH_TTL: int = 1800       # æœç´¢ç»“æœ30åˆ†é’Ÿ
    
    # Neo4jé…ç½®  
    NEO4J_URI: str = "bolt://localhost:7687"
    NEO4J_USER: str = "neo4j"
    NEO4J_PASSWORD: str = "password"
    NEO4J_MAX_CONNECTIONS: int = 20    # å¢åŠ è¿æ¥æ± 
    NEO4J_CONNECTION_TIMEOUT: int = 30 # è¿æ¥è¶…æ—¶
    
    # ç¼“å­˜ç­–ç•¥é…ç½®
    DATA_FRESHNESS_HOURS: int = 24     # æ•°æ®æ–°é²œåº¦
    ENABLE_PRELOAD: bool = True        # å¯ç”¨é¢„åŠ è½½
    BATCH_SIZE: int = 100              # æ‰¹å¤„ç†å¤§å°
    
    # ä»»åŠ¡é˜Ÿåˆ—é…ç½®
    ARQ_REDIS_URL: str = "redis://localhost:6379/1"
    ARQ_MAX_JOBS: int = 20             # å¢åŠ å¹¶å‘ä»»åŠ¡
    ARQ_JOB_TIMEOUT: int = 600         # ä»»åŠ¡è¶…æ—¶10åˆ†é’Ÿ
    ARQ_KEEP_RESULT: int = 3600        # ç»“æœä¿ç•™æ—¶é—´
    
    # ç›‘æ§é…ç½®
    ENABLE_METRICS: bool = True
    LOG_LEVEL: str = "INFO"
    METRICS_PORT: int = 8001
    HEALTH_CHECK_INTERVAL: int = 60
    
    class Config:
        env_file = ".env"
```

## ğŸ“š API æ–‡æ¡£ v1.0

### æ ¸å¿ƒ API æ¥å£

è¯¦ç»†çš„ API æ–‡æ¡£å°†é€šè¿‡ FastAPI è‡ªåŠ¨ç”Ÿæˆï¼Œè®¿é—® `/docs` æŸ¥çœ‹ Swagger UIã€‚

**ä¸»è¦æ¥å£åŒ…æ‹¬ï¼š**
- æ–‡çŒ®æŸ¥è¯¢ï¼š`GET /paper/{paper_id}` - æ”¯æŒå­—æ®µç­›é€‰
- å¼•ç”¨æŸ¥è¯¢ï¼š`GET /paper/{paper_id}/citations` - åˆ†é¡µæ”¯æŒ  
- å‚è€ƒæ–‡çŒ®ï¼š`GET /paper/{paper_id}/references` - åˆ†é¡µæ”¯æŒ
- æ–‡çŒ®æœç´¢ï¼š`GET /paper/search` - é«˜çº§æœç´¢
- æ‰¹é‡æŸ¥è¯¢ï¼š`POST /paper/batch` - æ‰¹é‡ä¼˜åŒ–
- å¥åº·æ£€æŸ¥ï¼š`GET /health` - è¯¦ç»†çŠ¶æ€
- ç³»ç»ŸæŒ‡æ ‡ï¼š`GET /metrics` - Prometheusæ ¼å¼

**v1.0 æ–°å¢åŠŸèƒ½ï¼š**
- å­—æ®µçº§ç¼“å­˜æ§åˆ¶
- æ•°æ®æ–°é²œåº¦æŒ‡ç¤º
- æ€§èƒ½æŒ‡æ ‡æš´éœ²
- æ‰¹é‡æ“ä½œä¼˜åŒ–

---

## ğŸ“ æ€»ç»“ v1.0

è¿™ä¸ª v1.0 æ¶æ„è®¾è®¡åœ¨ v0.4 åŸºç¡€ä¸Šè¿›è¡Œäº†é‡è¦ä¼˜åŒ–ï¼š

### ğŸ¯ æ ¸å¿ƒæ”¹è¿›

1. **å­˜å‚¨ç­–ç•¥ä¼˜åŒ–**ï¼š
   - å°æ•°æ®å†…è”å­˜å‚¨ï¼ˆmetadataç›´æ¥å­˜å‚¨åœ¨PaperèŠ‚ç‚¹ï¼‰
   - å¤§æ•°æ®åˆ†å—å­˜å‚¨ï¼ˆCitations/Referencesä½¿ç”¨DataChunkï¼‰
   - å‡å°‘æŸ¥è¯¢å¤æ‚åº¦ï¼Œæé«˜æ€§èƒ½

2. **ç¼“å­˜ç­–ç•¥å‡çº§**ï¼š
   - ä¸‰çº§ç¼“å­˜æ›´åŠ æ™ºèƒ½
   - æ•°æ®æ–°é²œåº¦æ„ŸçŸ¥
   - é¢„çƒ­å’Œå¤±æ•ˆç­–ç•¥ä¼˜åŒ–

3. **æ€§èƒ½æ˜¾è‘—æå‡**ï¼š
   - å“åº”æ—¶é—´ç›®æ ‡æ›´aggressive
   - ç¼“å­˜å‘½ä¸­ç‡ç›®æ ‡æ›´é«˜
   - ç³»ç»Ÿå®¹é‡è§„åˆ’æ›´åˆç†

4. **ç›‘æ§ä½“ç³»å®Œå–„**ï¼š
   - æ›´è¯¦ç»†çš„ä¸šåŠ¡æŒ‡æ ‡
   - ç»“æ„åŒ–æ—¥å¿—æ ¼å¼
   - å®Œæ•´çš„å‘Šè­¦è§„åˆ™

### ğŸš€ é¢„æœŸæ”¶ç›Š

- **æ€§èƒ½æå‡**ï¼šå¹³å‡å“åº”æ—¶é—´å‡å°‘50%+
- **æˆæœ¬é™ä½**ï¼šå­˜å‚¨å¼€é”€å‡å°‘30%+
- **å¯ç»´æŠ¤æ€§**ï¼šæŸ¥è¯¢å¤æ‚åº¦é™ä½ï¼Œè¿ç»´æ›´ç®€å•
- **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒæ›´å¤§è§„æ¨¡çš„æ•°æ®å’Œå¹¶å‘

é€šè¿‡è¿™ä¸ªv1.0è®¾è®¡ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªçœŸæ­£é«˜æ€§èƒ½ã€é«˜å¯ç”¨ã€æ˜“ç»´æŠ¤çš„å­¦æœ¯è®ºæ–‡ç¼“å­˜æœåŠ¡ã€‚

---

*æ–‡æ¡£ç‰ˆæœ¬ï¼šv1.0*  
*åˆ›å»ºæ—¶é—´ï¼š2025-09-17*  
*ç»´æŠ¤è€…ï¼šPaper Parser Team*
